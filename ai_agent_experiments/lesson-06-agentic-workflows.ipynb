{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# choosing between a simple script, a deterministic workflow, a traditional chatbot, a retrieval‐augmented generation (RAG) system, or a full‐blown autonomous agent\n",
    "\n",
    "To clarify this choice, consider four key *factors*:\n",
    "- the variability of your inputs,\n",
    "- the complexity of the reasoning required,\n",
    "- any performance or compliance constraints,\n",
    "- the ongoing maintenance burden."
   ],
   "id": "899c04020894e0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T04:57:53.958137Z",
     "start_time": "2025-10-09T04:57:53.954675Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "If your inputs are fully predictable and every possible output can be described in advance, a handful of lines of procedural code are often faster, cheaper, and far easier to test than an ML–based pipeline. For example, parsing a log file that always follows the format “YYYY‐MM‐DD HH:MM:SS—message” can be handled reliably with a small regular‐expression‐based parser in Python or Go. Likewise, if your application demands millisecond‐level latency—such as an embedded system that must react to sensor data in real time—there simply isn’t time for a language model API call. In such cases, traditional code is the right choice.\n",
    "\n",
    "regulated domains (medical devices, aeronautics, certain financial systems) often require fully deterministic, auditable decision logic—black‐box neural models won’t satisfy certification requirements.\n",
    "\n",
    "\n",
    "If any of these conditions hold\n",
    "- deterministic inputs,\n",
    "- strict performance\n",
    "- explainability needs\n",
    "- static problem domain\n",
    "plain code is almost always preferable to a foundation model."
   ],
   "id": "aa2f52df0565dd70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Deterministic workflows make sense whenever you can list all decision branches in advance and you need tight, auditable control over each branch. In such scenarios, workflows scale more naturally than large, ad hoc scripts but still avoid the complexity and cost of running an agentic pipeline.\n",
    "\n"
   ],
   "id": "a023b53a8947dfb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If your primary need is to let users ask questions about a knowledge base—say, searching a product manual, a legal archive, or corporate wikis—a RAG system can embed documents into a vector store, retrieve relevant passages in response to a query, and generate coherent, context‐aware answers.\n",
    "\n",
    "RAG systems do not independently decide on follow‐up actions (like filing a ticket or scheduling a callback); they simply surface information. A traditional chatbot or RAG approach makes sense when the task is primarily question‐answering over structured or unstructured content, with limited need for external API calls or decision orchestration\n",
    "\n",
    "Maintenance costs are lower than for agents—your main overhead lies in keeping document embeddings up to date and refining prompts—but you sacrifice the agent’s ability to plan multistep workflows or learn from feedback loops."
   ],
   "id": "ea8060c9e90fed98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Need for Agentic flows come when inputs are unstructured, novel, or highly variable, and because you require dynamic, multistep planning or continuous learning from feedback.\n",
   "id": "ed53a00893d2897c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Degrees of Autonomy\n",
    "In an agentic workflow, the degrree of autonomy can vary significantly\n",
    "\n",
    "Simple agentic workflow like `write an essay of Blackholes` is a low-autonomy task, where the agent is given a specific task and is expected to complete it with minimal intervention. We might have simple deterministic steps to follow, such as\n",
    "* [LLM] write the query to search for the topic\n",
    "* websearch the topic,\n",
    "* Webscrape the content,\n",
    "* [LLM] writing the content\n",
    "\n",
    "More autonomous workflow for same task might involve\n",
    "* [LLM] Determine which tool to use to lookup the topic,\n",
    "* [LLM] once decided to use websearch, write the query to search for the topic,\n",
    "* websearch the topic,\n",
    "* [LLM] Determine the tool to use like pdf2txt, webfetch\n",
    "* Webscrape the content,\n",
    "* [LLM] outlining the essay,\n",
    "* [LLM] writing the content (draft)\n",
    "* [LLM] proofreading and improve the draft\n",
    "* [LLM] writing the final essay.\n",
    "\n",
    "## Task Decomposition\n",
    "\n",
    "![image.png](../resources/img.png)"
   ],
   "id": "e3d67777f033d33b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reflection Pattern\n",
    "Agentic workflows are powerful, but they can also be unpredictable. To mitigate this, consider implementing a reflection pattern, where the agent periodically reviews its actions and decisions. This can help catch errors early and ensure that the workflow stays on track."
   ],
   "id": "fad99e8f7d6566ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "== Chart Generation Agentic Workflow\n",
    "\n",
    "- Generate an initial version (V1): Use a Large Language Model (LLM) to create the first version of the plotting code.\n",
    "- Execute code and create chart: Run the generated code and display the resulting chart. _(check everywhere)_\n",
    "- Reflect on the output: Evaluate both the code and the chart using an LLM to detect areas for improvement (e.g., clarity, accuracy, design).\n",
    "- Generate and execute improved version (V2): Produce a refined version of the plotting code based on reflection insights and render the enhanced chart."
   ],
   "id": "227a160979fb5834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import json\n",
    "\n",
    "from numpy.random import sample\n",
    "\n",
    "# Local helper module\n",
    "import utils\n"
   ],
   "id": "c95903e0ab4b09b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = utils.load_and_prepare_data(\"../resources/1.csv\")\n",
    "utils.print_html(df.sample(5), title=\"Sample Coffee Sales Data\")"
   ],
   "id": "982969ad89e4ab61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_chart_code(instruction: str, model: str, out_path_v1: str) -> str:\n",
    "    \"\"\"Generate Python code to make a plot with matplotlib using tag-based wrapping.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data visualization expert.\n",
    "\n",
    "    Return your answer *strictly* in this format:\n",
    "\n",
    "    <execute_python>\n",
    "    # valid python code here\n",
    "    </execute_python>\n",
    "\n",
    "    Do not add explanations, only the tags and the code.\n",
    "\n",
    "    The code should create a visualization from a DataFrame 'df' with these columns:\n",
    "    - date (M/D/YY)\n",
    "    - time (HH:MM)\n",
    "    - cash_type (card or cash)\n",
    "    - card (string)\n",
    "    - price (number)\n",
    "    - coffee_name (string)\n",
    "    - quarter (1-4)\n",
    "    - month (1-12)\n",
    "    - year (YYYY)\n",
    "\n",
    "    User instruction: {instruction}\n",
    "\n",
    "    Requirements for the code:\n",
    "    1. Assume the DataFrame is already loaded as 'df'.\n",
    "    2. Use matplotlib for plotting.\n",
    "    3. Add clear title, axis labels, and legend if needed.\n",
    "    4. Save the figure as '{out_path_v1}' with dpi=300.\n",
    "    5. Do not call plt.show().\n",
    "    6. Close all plots with plt.close().\n",
    "    7. Add all necessary import python statements\n",
    "\n",
    "    Return ONLY the code wrapped in <execute_python> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    response = utils.get_response(model, prompt)\n",
    "    return response"
   ],
   "id": "74d1b0cb5f6dea7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate initial code\n",
    "code_v1 = generate_chart_code(\n",
    "    instruction=\"Create a plot comparing Q1 coffee sales in 2024 and 2025 using the data in coffee_sales.csv.\",\n",
    "    model=\"gpt-4.1-dz\",\n",
    "    out_path_v1=\"chart_v1.png\"\n",
    ")\n",
    "\n",
    "utils.print_html(code_v1, title=\"LLM output with first draft code\")"
   ],
   "id": "d748db254218d165",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "match = re.search(r\"<execute_python>([\\s\\S]*?)</execute_python>\", code_v1)\n",
    "if match:\n",
    "    initial_code = match.group(1).strip()\n",
    "    utils.print_html(initial_code, title=\"Extracted Code to Execute\")\n",
    "    exec_globals = {\"df\": df}\n",
    "    exec(initial_code, exec_globals)\n",
    "\n",
    "# If code run successfully, the file chart_v1.png should have been generated\n",
    "utils.print_html(\n",
    "    content=\"chart_v1.png\",\n",
    "    title=\"Generated Chart (V1)\",\n",
    "    is_image=True\n",
    ")"
   ],
   "id": "792e6f94d1c811d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reflect_on_image_and_regenerate(\n",
    "    chart_path: str,\n",
    "    instruction: str,\n",
    "    model_name: str,\n",
    "    out_path_v2: str,\n",
    "    code_v1: str,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Critique the chart IMAGE and the original code against the instruction,\n",
    "    then return refined matplotlib code.\n",
    "    Returns (feedback, refined_code_with_tags).\n",
    "    Supports OpenAI and Anthropic (Claude).\n",
    "    \"\"\"\n",
    "    media_type, b64 = utils.encode_image_b64(chart_path)\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data visualization expert.\n",
    "    Your task: critique the attached chart and the original code against the given instruction,\n",
    "    then return improved matplotlib code.\n",
    "\n",
    "    Original code (for context):\n",
    "    {code_v1}\n",
    "\n",
    "    OUTPUT FORMAT (STRICT):\n",
    "    1) First line: a valid JSON object with ONLY the \"feedback\" field.\n",
    "    Example: {{\"feedback\": \"The legend is unclear and the axis labels overlap.\"}}\n",
    "\n",
    "    2) After a newline, output ONLY the refined Python code wrapped in:\n",
    "    <execute_python>\n",
    "    ...\n",
    "    </execute_python>\n",
    "\n",
    "    3) Import all necessary libraries in the code. Don't assume any imports from the original code.\n",
    "\n",
    "    HARD CONSTRAINTS:\n",
    "    - Do NOT include Markdown, backticks, or any extra prose outside the two parts above.\n",
    "    - Use pandas/matplotlib only (no seaborn).\n",
    "    - Assume df already exists; do not read from files.\n",
    "    - Save to '{out_path_v2}' with dpi=300.\n",
    "    - Always call plt.close() at the end (no plt.show()).\n",
    "    - Include all necessary import statements.\n",
    "\n",
    "    Schema (columns available in df):\n",
    "    - date (M/D/YY)\n",
    "    - time (HH:MM)\n",
    "    - cash_type (card or cash)\n",
    "    - card (string)\n",
    "    - price (number)\n",
    "    - coffee_name (string)\n",
    "    - quarter (1-4)\n",
    "    - month (1-12)\n",
    "    - year (YYYY)\n",
    "\n",
    "    Instruction:\n",
    "    {instruction}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # In case the name is \"Claude\" or \"Anthropic\", use the safe helper\n",
    "    lower = model_name.lower()\n",
    "    if \"claude\" in lower or \"anthropic\" in lower:\n",
    "        # ✅ Use the safe helper that joins all text blocks and adds a system prompt\n",
    "        content = utils.image_anthropic_call(model_name, prompt, media_type, b64)\n",
    "    else:\n",
    "        content = utils.image_openai_call(model_name, prompt, media_type, b64)\n",
    "\n",
    "    # --- Parse ONLY the first JSON line (feedback) ---\n",
    "    lines = content.strip().splitlines()\n",
    "    json_line = lines[0].strip() if lines else \"\"\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(json_line)\n",
    "    except Exception as e:\n",
    "        # Fallback: try to capture the first {...} in all the content\n",
    "        m_json = re.search(r\"\\{.*?\\}\", content, flags=re.DOTALL)\n",
    "        if m_json:\n",
    "            try:\n",
    "                obj = json.loads(m_json.group(0))\n",
    "            except Exception as e2:\n",
    "                obj = {\"feedback\": f\"Failed to parse JSON: {e2}\", \"refined_code\": \"\"}\n",
    "        else:\n",
    "            obj = {\"feedback\": f\"Failed to find JSON: {e}\", \"refined_code\": \"\"}\n",
    "\n",
    "    # --- Extract refined code from <execute_python>...</execute_python> ---\n",
    "    m_code = re.search(r\"<execute_python>([\\s\\S]*?)</execute_python>\", content)\n",
    "    refined_code_body = m_code.group(1).strip() if m_code else \"\"\n",
    "    refined_code = utils.ensure_execute_python_tags(refined_code_body)\n",
    "\n",
    "    feedback = str(obj.get(\"feedback\", \"\")).strip()\n",
    "    return feedback, refined_code\n"
   ],
   "id": "fa07fd3c3f375442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate feedback alongside reflected code\n",
    "feedback, code_v2 = reflect_on_image_and_regenerate(\n",
    "    chart_path=\"chart_v1.png\",\n",
    "    instruction=\"Create a plot comparing Q1 coffee sales in 2024 and 2025 using the data in coffee_sales.csv.\",\n",
    "    model_name=\"o4-mini\",\n",
    "    out_path_v2=\"chart_v2.png\",\n",
    "    code_v1=code_v1,   # pass in the original code for context\n",
    ")\n",
    "\n",
    "utils.print_html(feedback, title=\"Feedback on V1 Chart\")\n",
    "utils.print_html(code_v2, title=\"Regenerated Code Output (V2)\")"
   ],
   "id": "fdde55cec7da5bab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the code within the <execute_python> tags\n",
    "match = re.search(r\"<execute_python>([\\s\\S]*?)</execute_python>\", code_v2)\n",
    "if match:\n",
    "    reflected_code = match.group(1).strip()\n",
    "    exec_globals = {\"df\": df}\n",
    "    exec(reflected_code, exec_globals)\n",
    "\n",
    "# If code run successfully, the file chart_v2.png should have been generated\n",
    "utils.print_html(\n",
    "    content=\"chart_v2.png\",\n",
    "    title=\"Regenerated Chart (V2)\",\n",
    "    is_image=True\n",
    ")"
   ],
   "id": "24cc809cacb67efd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_workflow(\n",
    "    dataset_path: str,\n",
    "    user_instructions: str,\n",
    "    generation_model: str,\n",
    "    reflection_model: str,\n",
    "    image_basename: str = \"chart\",\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline:\n",
    "      1) load dataset\n",
    "      2) generate V1 code\n",
    "      3) execute V1 → produce chart_v1.png\n",
    "      4) reflect on V1 (image + original code) → feedback + refined code\n",
    "      5) execute V2 → produce chart_v2.png\n",
    "\n",
    "    Returns a dict with all artifacts (codes, feedback, image paths).\n",
    "    \"\"\"\n",
    "    # 0) Load dataset; utils handles parsing and feature derivations (e.g., year/quarter)\n",
    "    df = utils.load_and_prepare_data(dataset_path)\n",
    "    utils.print_html(df.sample(n=5), title=\"Random Sample of Dataset\")\n",
    "\n",
    "    # Paths to store charts\n",
    "    out_v1 = f\"{image_basename}_v1.png\"\n",
    "    out_v2 = f\"{image_basename}_v2.png\"\n",
    "\n",
    "    # 1) Generate code (V1)\n",
    "    utils.print_html(\"Step 1: Generating chart code (V1)… 📈\")\n",
    "    code_v1 = generate_chart_code(\n",
    "        instruction=user_instructions,\n",
    "        model=generation_model,\n",
    "        out_path_v1=out_v1,\n",
    "    )\n",
    "    utils.print_html(code_v1, title=\"LLM output with first draft code (V1)\")\n",
    "\n",
    "    # 2) Execute V1 (hard-coded: extract <execute_python> block and run immediately)\n",
    "    utils.print_html(\"Step 2: Executing chart code (V1)… 💻\")\n",
    "    match = re.search(r\"<execute_python>([\\s\\S]*?)</execute_python>\", code_v1)\n",
    "    if match:\n",
    "        initial_code = match.group(1).strip()\n",
    "        exec_globals = {\"df\": df}\n",
    "        exec(initial_code, exec_globals)\n",
    "    utils.print_html(out_v1, is_image=True, title=\"Generated Chart (V1)\")\n",
    "\n",
    "    # 3) Reflect on V1 (image + original code) to get feedback and refined code (V2)\n",
    "    utils.print_html(\"Step 3: Reflecting on V1 (image + code) and generating improvements… 🔁\")\n",
    "    feedback, code_v2 = reflect_on_image_and_regenerate(\n",
    "        chart_path=out_v1,\n",
    "        instruction=user_instructions,\n",
    "        model_name=reflection_model,\n",
    "        out_path_v2=out_v2,\n",
    "        code_v1=code_v1,  # pass original code for context\n",
    "    )\n",
    "    utils.print_html(feedback, title=\"Reflection feedback on V1\")\n",
    "    utils.print_html(code_v2, title=\"LLM output with revised code (V2)\")\n",
    "\n",
    "    # 4) Execute V2 (hard-coded: extract <execute_python> block and run immediately)\n",
    "    utils.print_html(\"Step 4: Executing refined chart code (V2)… 🖼️\")\n",
    "    match = re.search(r\"<execute_python>([\\s\\S]*?)</execute_python>\", code_v2)\n",
    "    if match:\n",
    "        reflected_code = match.group(1).strip()\n",
    "        exec_globals = {\"df\": df}\n",
    "        exec(reflected_code, exec_globals)\n",
    "    utils.print_html(out_v2, is_image=True, title=\"Regenerated Chart (V2)\")\n",
    "\n",
    "    return {\n",
    "        \"code_v1\": code_v1,\n",
    "        \"chart_v1\": out_v1,\n",
    "        \"feedback\": feedback,\n",
    "        \"code_v2\": code_v2,\n",
    "        \"chart_v2\": out_v2,\n",
    "    }\n"
   ],
   "id": "5f11c30a71ec91f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Here, insert your updates\n",
    "user_instructions=\"Create a plot comparing Q1 coffee sales in 2024 and 2025 using the data in coffee_sales.csv by coffee name\" # write your instruction here\n",
    "generation_model=\"gpt-4o-mini\"\n",
    "reflection_model=\"o4-mini\"\n",
    "image_basename=\"drink_sales\"\n",
    "\n",
    "# Run the complete agentic workflow\n",
    "_ = run_workflow(\n",
    "    dataset_path=\"../resources/1.csv\",\n",
    "    user_instructions=user_instructions,\n",
    "    generation_model=generation_model,\n",
    "    reflection_model=reflection_model,\n",
    "    image_basename=image_basename\n",
    ")"
   ],
   "id": "6656c8c52ef92828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8f711830f171fd9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
