{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyexpat.errors import messages\n",
    "!poetry remove wikipedia\n",
    "!poetry add wikipedia-api"
   ],
   "id": "69dc9223399bfd9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!poetry add numpy",
   "id": "6610e311971ee6c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from ai_agent_experiments.config import Configuration\n",
    "\n",
    "config=Configuration(\"./config.json\")\n",
    "messages=[{\"role\":\"system\", \"content\":\"You are a Helpful person who has deep interest in speaking history in the form of anecdotes.\"}]"
   ],
   "id": "f8992488a00cee08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import AzureOpenAI, embeddings\n",
    "\n",
    "client= AzureOpenAI(api_key= config.client_config[\"api_key\"], azure_endpoint=config.client_config[\"azure_endpoint\"],api_version=config.client_config[\"api_version\"] )\n",
    "response=client.chat.completions.create(model=config.client_config[\"model\"],messages=messages)\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "bd8660cfe7efd569",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wikipediaapi import Wikipedia\n",
    "wiki = Wikipedia(\"RAGBot/0.0\", 'en')\n",
    "doc_page = wiki.page(\"Python_(programming_language)\").text\n",
    "chunks = doc_page.split(\"\\n\\n\") # Splitting by double newlines for paragraphs\n",
    "\n"
   ],
   "id": "49bd280af1980eab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embeddings=client.embeddings.create(input=chunks, model=\"text-embedding-ada-002\")",
   "id": "cc378ed8ed7be6e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(embeddings.data[0].embedding)",
   "id": "16d6730899ba303e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_query=\"tell me the history of python programming language\"\n",
    "user_query_embedding=client.embeddings.create(input=[user_query], model=\"text-embedding-ada-002\")"
   ],
   "id": "461c993790539e34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_most_similar_chunks_numpy(query_vector, chunk_vectors, chunks, top_k=3):\n",
    "    \"\"\"\n",
    "    Pure NumPy implementation - no external dependencies\n",
    "    \"\"\"\n",
    "    query = np.array(query_vector)\n",
    "    chunk_arrays = np.array(chunk_vectors)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = np.dot(chunk_arrays, query) / (\n",
    "        np.linalg.norm(chunk_arrays, axis=1) * np.linalg.norm(query)\n",
    "    )\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'similarity': similarities[idx],\n",
    "            'index': idx\n",
    "        })\n",
    "\n",
    "    return results\n"
   ],
   "id": "f757270f58fe3ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract embeddings from the response objects\n",
    "chunk_vectors = [item.embedding for item in embeddings.data]\n",
    "query_vector = user_query_embedding.data[0].embedding\n",
    "\n",
    "top_chunks = find_most_similar_chunks_numpy(query_vector, chunk_vectors, chunks, top_k=3)"
   ],
   "id": "a0cec6e4491b8343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "top_chunks",
   "id": "3f98d5611b3eb1ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_rag_response(openai_client, user_input, retrieved_chunks):\n",
    "    content_chunks = \"\\n\\n\".join([chunk['chunk'] for chunk in retrieved_chunks])\n",
    "    updated_content = f\"\"\"Based on the following context, answer the user's question. If the answer cannot be found in the context, say so.\n",
    "\n",
    "    Context:{content_chunks}\n",
    "    Question: {user_query}\n",
    "    \"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": updated_content})\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=config.client_config[\"model\"],\n",
    "        messages=messages)\n",
    "    return response.choices[0].message.content"
   ],
   "id": "1221fd6d231aaea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(generate_rag_response(client, user_query, top_chunks))",
   "id": "87a69de6a508c8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4e3ce3283d47c477",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
